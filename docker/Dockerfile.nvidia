##############################################################
# This Dockerfile contains the NVidia HPC SDK (nvc, cuda, OpenMPI) for Devito
##############################################################
ARG ver
ARG arch="nvc"

########################################################################
# Build base image with apt setup and common env
########################################################################
FROM ubuntu:22.04 AS sdk-base

SHELL ["/bin/bash", "-c"]

ENV DEBIAN_FRONTEND=noninteractive

# Install python
RUN apt-get update && \
    apt-get install -y -q gpg apt-utils curl libnuma-dev cmake git \
                          dh-autoreconf python3-venv python3-dev python3-pip

# nodesource: nvdashboard requires nodejs>=10
SHELL /bin/bash -o pipefail
RUN curl https://developer.download.nvidia.com/hpc-sdk/ubuntu/DEB-GPG-KEY-NVIDIA-HPC-SDK | gpg --yes --dearmor -o /usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg
RUN arch="$(uname -m)" && \
    case "$arch" in \
        x86_64) nvplat=amd64 ;; \
        aarch64|arm64) nvplat=arm64 ;; \
        *) echo "Unsupported architecture: $arch" >&2; exit 1 ;; \
    esac && \
    echo "deb [trusted=yes, signed-by=/usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg] https://developer.download.nvidia.com/hpc-sdk/ubuntu/${nvplat} /" | tee /etc/apt/sources.list.d/nvhpc.list
RUN apt-key update -- * && apt-get update -y

#Â Install nvhpc. `nvhpc` is the alias for the latest available version
ARG ver=nvhpc
# We use the standard apt-get for the default latest nvhpc. For earlier version, apt has a bug that it will always
# install the latest nvhpc-x-y no matter which version nvhpc-x-z is requested which would double (extra 10Gb) the size of the image.
# So for specific version we directly download the specific deb and install it.
RUN arch="$(uname -m)" && \
    case "$arch" in \
        x86_64) nvplat=amd64 ;; \
        aarch64|arm64) nvplat=arm64 ;; \
        *) echo "Unsupported architecture: $arch" >&2; exit 1 ;; \
    esac && \
    if [ "$ver" = "nvhpc" ]; then \
        apt-get install -y -q --allow-unauthenticated ${ver}; \
    else \
        year=$(echo $ver | cut -d "-" -f 2) && \
        export year && \
        minor=$(echo $ver | cut -d "-" -f 3) && \
        export minor && \
        curl -O nvhpc.deb -L "https://developer.download.nvidia.com/hpc-sdk/ubuntu/${nvplat}/nvhpc_${year}.${minor}_${nvplat}.deb" \
        || curl -O nvhpc.deb -L "https://developer.download.nvidia.com/hpc-sdk/ubuntu/${nvplat}/nvhpc_${year}.${minor}-0_${nvplat}.deb" && \
        apt-get install --allow-unauthenticated -y -q ./nvhpc.deb; \
    fi;

# Nodejs https://github.com/nodesource/distributions
RUN curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --yes --dearmor -o /etc/apt/keyrings/nodesource.gpg
RUN echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_18.x nodistro main" | tee /etc/apt/sources.list.d/nodesource.list
RUN apt-get update && apt-get install -y -q \
        liblapack-dev libblas-dev \
        libibverbs-dev libmlx4-1 libmlx5-1 ibutils \
        # Devito Jupyter Notebooks and Ux experience
        nodejs ffmpeg \
        texlive-latex-extra texlive-fonts-recommended dvipng cm-super

# nvptx only available on x86_64
RUN apt-get install -y -q gcc-offload-nvptx || echo "Skipping nvptx compiler installation on non-x86_64 architecture"

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# MPI ROOT USER DEFAULTS
ENV OMPI_ALLOW_RUN_AS_ROOT=1
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
ENV OMPI_MCA_rmaps_base_oversubscribe=1
ENV OMPI_MCA_btl_base_warn_component_unused=0
ENV OMPI_MCA_hwloc_base_binding_policy=""
ENV UCX_MEMTYPE_CACHE=no
ENV UCX_NET_DEVICES=all
ENV UCX_SHM_DEVICES=all
ENV UCX_ACC_DEVICES=all
ENV UCX_RNDV_THRESH=0
ENV UCX_RNDV_SCHEME=get_zcopy
ENV NCCL_UCX_RNDV_THRESH=0
ENV NCCL_UCX_RNDV_SCHEME=get_zcopy
ENV NCCL_PLUGIN_P2P=ucx
ENV MELLANOX_MOUNT_DRIVER=1

ENV UCX_TLS=cuda,cuda_copy,cuda_ipc,sm,shm,self
# For Baremetal, these flags are also available
#ENV UCX_TLS=cuda,cuda_copy,cuda_ipc,sm,shm,self,rc_x,gdr_copy

# Make simlink for path setup since ENV doesn't accept shell commands.
SHELL /bin/bash -o pipefail
RUN arch="$(uname -m)" && \
    case "$arch" in \
        x86_64) linux=Linux_x86_64 ;; \
        aarch64|arm64) linux=Linux_aarch64 ;; \
        *) echo "Unsupported architecture: $arch" >&2; exit 1 ;; \
    esac && \
    export NVARCH=$(ls -1 /opt/nvidia/hpc_sdk/${linux}/ | grep '\.' | head -n 1) && \
    export CUDA_V=$(ls /opt/nvidia/hpc_sdk/${linux}/${NVARCH}/cuda/ | grep '\.') && \
    ln -sf /opt/nvidia/hpc_sdk/${linux}/${NVARCH} /opt/nvhpc && \
    ln -sf /opt/nvidia/hpc_sdk/${linux}/${NVARCH}/cuda/${CUDA_V}/extras/CUPTI /opt/CUPTI && \
    ln -sf /opt/nvidia/hpc_sdk/${linux}/comm_libs/${CUDA_V}/nvshmem /opt/nvhpc/comm_libs/nvshmem && \
    ln -sf /opt/nvidia/hpc_sdk/${linux}/comm_libs/${CUDA_V}/nccl /opt/nvhpc/comm_libs/nccl && \
    ln -sf /opt/nvidia/hpc_sdk/${linux}/${NVARCH}/cuda/${CUDA_V}/compute-sanitizer/compute-sanitizer /opt/nvhpc/compilers/bin/compute-sanitizer

# Starting nvhpc 23.5 and cuda 12.1, hpcx and openmpi are inside the cuda version folder, only the bin is in the comm_libs path
RUN export CUDA_V=$(/opt/nvhpc/cuda/bin/nvcc --version | sed -n 's/^.*release \([0-9]\+\.[0-9]\+\).*$/\1/p') && \
    ls /opt/nvhpc/comm_libs/${CUDA_V}/hpcx/ &&\
    if [ -d /opt/nvhpc/comm_libs/${CUDA_V}/hpcx ]; then \
        rm -rf /opt/nvhpc/comm_libs/hpcx && rm -rf /opt/nvhpc/comm_libs/openmpi4 && \
        ln -sf /opt/nvhpc/comm_libs/${CUDA_V}/hpcx /opt/nvhpc/comm_libs/hpcx && \
        ln -sf /opt/nvhpc/comm_libs/${CUDA_V}/openmpi4 /opt/nvhpc/comm_libs/openmpi4;\
    fi;
# Set base path based on version
ENV HPCSDK_HOME=/opt/nvhpc
ENV HPCSDK_CUPTI=/opt/CUPTI

# required for nvidia-docker v1
RUN echo "$HPCSDK_HOME/cuda/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/cuda/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/compilers/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/comm_libs/mpi/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_CUPTI/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/math_libs/lib64" >> /etc/ld.so.conf.d/nvidia.conf

# Compiler, CUDA, and Library paths
# CUDA_HOME has been deprecated but keep for now because of other dependencies (@mloubout).
ENV CUDA_HOME=$HPCSDK_HOME/cuda
ENV NVHPC_CUDA_HOME=$HPCSDK_HOME/cuda
ENV CUDA_ROOT=$HPCSDK_HOME/cuda/bin
ENV PATH=$HPCSDK_HOME/compilers/bin:$HPCSDK_HOME/cuda/bin:$HPCSDK_HOME/comm_libs/mpi/bin:${PATH}
ENV LD_LIBRARY_PATH=$HPCSDK_HOME/cuda/lib:$HPCSDK_HOME/cuda/lib64:$HPCSDK_HOME/compilers/lib:$HPCSDK_HOME/math_libs/lib64:$HPCSDK_HOME/comm_libs/mpi/lib:$HPCSDK_CUPTI/lib64:bitcomp_DIR
ENV CPATH=$HPCSDK_HOME/comm_libs/mpi/include:$HPCSDK_HOME/comm_libs/nvshmem/include:$HPCSDK_HOME/comm_libs/nccl/include:$HPCSDK_HOME/math_libs/include

# MPI
RUN rm -f  $HPCSDK_HOME/comm_libs/mpi && \
    ln -sf $HPCSDK_HOME/comm_libs/hpcx/latest/ompi $HPCSDK_HOME/comm_libs/mpi

# Install python nvidia dependencies
RUN python3 -m venv /venv && \
    /venv/bin/pip install --no-cache-dir --upgrade pip && \
    /venv/bin/pip install --no-cache-dir -r https://raw.githubusercontent.com/devitocodes/devito/main/requirements-nvidia.txt && \
    # Install jupyter and setup nvidia configs.
    /venv/bin/pip install --no-cache-dir jupyter && \
    /venv/bin/jupyter server extension enable dask_labextension && \
    rm -rf ~/.cache/pip

RUN apt-get clean && apt-get autoclean && apt-get autoremove -y && \
    rm -rf /var/lib/apt/lists/*

EXPOSE 8888
CMD ["/bin/bash"]

########################################################################
# NVC for GPUs via OpenACC config
########################################################################
FROM sdk-base AS nvc

# Make devito env vars file and extras
COPY docker/nvdashboard.json /app/nvdashboard.json

# mpi4py
ENV MPI4PY_FLAGS='source $HPCSDK_HOME/comm_libs/hpcx/latest/hpcx-init.sh && hpcx_load && CC=nvc CFLAGS="-noswitcherror -tp=px"'

ENV DEVITO_ARCH="nvc"
ENV DEVITO_PLATFORM="nvidiaX"
ENV DEVITO_LANGUAGE="openacc"

########################################################################
# NVC for GPUs via CUDA config
########################################################################
FROM nvc AS nvcc

ENV DEVITO_ARCH="cuda"
ENV DEVITO_PLATFORM="nvidiaX"
ENV DEVITO_LANGUAGE="cuda"

########################################################################
# NVC for CPUs config
########################################################################
FROM nvc AS nvc-host

ENV DEVITO_ARCH="nvc"
ENV DEVITO_PLATFORM="cpu64"
ENV DEVITO_LANGUAGE="openmp"
