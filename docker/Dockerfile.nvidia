##############################################################
# This Dockerfile contains the additional NVIDIA compilers, 
# libraries, and plugins to enable OpenACC and NVIDIA GPU 
# acceleration of Devito codes.
#
# BUILD: 
#   docker build --network=host --file docker/Dockerfile.nvidia --tag devito:nvidia .
#
# LEGACY:
#   (1) Option MPI 3.0:
#   docker build --network=host --build-arg MPI_VER=3 --file docker/Dockerfile.nvidia --tag devito:nvidia .
#
#   (2) Option MPI 4.0:
#   Enabling and using MPI 4.0.5 works on R450 drivers, but is showing compatibility 
#   issues during testing on older R418 drivers.
# 
#   docker build --network=host --build-arg MPI_VER=4 --file docker/Dockerfile.nvidia --tag devito:nvidia .
#
# SQUASH:
#   If you want a smaller image or to collapse/combine the smaller layers, try squashing your docker image
#
#   pip install docker-squash
#   docker tag devito:nvidia devito:nvidia-pre
#   docker-squash  -t devito:nvidia devito:nvidia-pre
#
# EXPORT:
#   docker save devito:nvidia | gzip > devito_nvidia.tar.gz
#   docker load < devito_nvidia.tar.gz
#
# RUN: 
#   docker run --gpus all --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 devito:nvidia
#   docker run --gpus all --rm -it devito:nvidia python examples/seismic/acoustic/acoustic_example.py
#
#   docker run --gpus all --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 --device=/dev/infiniband/uverbs0 --device=/dev/infiniband/rdma_cm  devito:nvidia 
#
# to run in user context on a cluster with shared filesystem, you can add the correct user config as docker options e.g.:
#   docker run --gpus all --rm -it -v `pwd`:`pwd` -w `pwd` -u $(id -u):$(id -g) devito:nvidia python examples/seismic/acoustic/acoustic_example.py
#
##############################################################
#FROM python:3.9 
FROM python:3.9-slim-bullseye 
 
ENV DEBIAN_FRONTEND noninteractive 
 
ARG HPCSDK=22-5
ARG HPCSDK_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/2022
ARG HPCSDK_CUPTI=/opt/nvidia/hpc_sdk/Linux_x86_64/2022/cuda/11.7/extras/CUPTI
#MPI_VER options 3,4,HPCX
ARG MPI_VER=HPCX
 
# nodesource: nvdashboard requires nodejs>=10  
RUN echo 'deb [trusted=yes] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' > /etc/apt/sources.list.d/nvhpc.list && \
    apt-get update -y && \
    apt-get install -y -q \
        apt-utils curl wget && \   
    curl -sL https://deb.nodesource.com/setup_18.x | bash - && \
    apt-get install -y -q \
#        nvhpc-$HPCSDK-cuda-multi \
        nvhpc-$HPCSDK \
        liblapack-dev libblas-dev \
        libibverbs-dev libmlx4-1 libmlx5-1 ibutils \
        gcc-offload-nvptx \
# Devito Jupyter Notebooks and Ux experience
        nodejs \
        texlive-latex-extra texlive-fonts-recommended dvipng cm-super \
        ffmpeg vim && \
    wget -q -P /app/nvcomp_exts/ \
        https://developer.download.nvidia.com/compute/nvcomp/2.3.2/local_installers/nvcomp_install_CUDA_11.x.tgz && \
    tar -xvf /app/nvcomp_exts/nvcomp_install_CUDA_11.x.tgz -C /app/nvcomp_exts && \
# Cleanup HPC SDK install to reduce size
    rm -rf $HPCSDK_HOME/examples/* && \
    rm -rf $HPCSDK_HOME/profilers/* && \ 
    rm -rf $HPCSDK_HOME/../../modulefiles && \
    rm -rf $HPCSDK_HOME/../../REDIST && \
    rm -rf $HPCSDK_HOME/../../examples && \
    rm -rf $HPCSDK_HOME/../../profilers && \
# Cleanup image to reduce size
    apt-get update -y && \
    apt-get autoclean && \
    apt-get autoremove && \
    rm -rf /app/nvcomp_exts/nvcomp* && \
    rm -rf /var/lib/apt/lists/* 

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility

# required for nvidia-docker v1
RUN echo "$HPCSDK_HOME/cuda/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/cuda/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/compilers/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/comm_libs/mpi/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_CUPTI/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/math_libs/lib64" >> /etc/ld.so.conf.d/nvidia.conf    

# Compression
ENV NVCOMP_EXTS_ROOT /app/nvcomp_exts
ENV bitcomp_DIR $NVCOMP_EXTS_ROOT/lib/
    
# Compiler, CUDA, and Library paths
ENV CUDA_HOME $HPCSDK_HOME/cuda
ENV CUDA_ROOT $HPCSDK_HOME/cuda/bin
ENV PATH $HPCSDK_HOME/compilers/bin:$HPCSDK_HOME/cuda/bin:$HPCSDK_HOME/comm_libs/mpi/bin:${PATH}
ENV LD_LIBRARY_PATH $HPCSDK_HOME/cuda/lib:$HPCSDK_HOME/cuda/lib64:$HPCSDK_HOME/compilers/lib:$HPCSDK_HOME/math_libs/lib64:$HPCSDK_HOME/comm_libs/mpi/lib:$HPCSDK_CUPTI/lib64:bitcomp_DIR:${LD_LIBRARY_PATH}

# Copy Devito
ADD . /app/devito
# Cleanup git files
RUN rm -rf /app/devito/.git

# add entrypoint file to container for customization
ADD docker/entrypoint.sh /docker-entrypoint.sh
RUN chmod +x /docker-entrypoint.sh 

# MPI ROOT USER DEFAULTS
ENV OMPI_ALLOW_RUN_AS_ROOT=1
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
ENV OMPI_MCA_rmaps_base_oversubscribe=1
ENV OMPI_MCA_btl_base_warn_component_unused=0
ENV OMPI_MCA_hwloc_base_binding_policy=""
ENV UCX_MEMTYPE_CACHE=no
ENV UCX_NET_DEVICES=all
ENV UCX_SHM_DEVICES=all
ENV UCX_ACC_DEVICES=all
ENV NCCL_UCX_RNDV_THRESH=0
ENV NCCL_UCX_RNDV_SCHEME=get_zcopy
ENV NCCL_PLUGIN_P2P=ucx
ENV MELLANOX_MOUNT_DRIVER=1

ENV UCX_TLS=cuda,cuda_copy,cuda_ipc,sm,shm,self
# For Baremetal, these flags are also available
#ENV UCX_TLS=cuda,cuda_copy,cuda_ipc,sm,shm,self,rc_x,gdr_copy

ENV CPATH $HPCSDK_HOME/comm_libs/mpi/include:${CPATH}

# MPI 3
#    Do Nothing
RUN if [ "x$MPI_VER" = "x4" ]; then \
        rm -f  $HPCSDK_HOME/comm_libs/mpi && \
        ln -sf $HPCSDK_HOME/comm_libs/openmpi4/openmpi-4.0.5 \
               $HPCSDK_HOME/comm_libs/mpi ; \
    fi;  \
    if [ "x$MPI_VER" = "xHPCX" ]; then \
        rm -f  $HPCSDK_HOME/comm_libs/mpi && \
        ln -sf $HPCSDK_HOME/comm_libs/hpcx/latest/ompi \
               $HPCSDK_HOME/comm_libs/mpi ; \
    fi;

# Update entrypoint.sh with hpcx setup 
RUN \
    if [ "x$MPI_VER" = "xHPCX" ]; then \
        sed -i "s|#<->|source $HPCSDK_HOME/comm_libs/hpcx/latest/hpcx-init.sh\n#<->|g" /docker-entrypoint.sh && \
        sed -i 's|#<->|hpcx_load\n#<->|g' /docker-entrypoint.sh ; \
    fi;

#Â Install pip dependencies and devito as a pip package
RUN python3 -m venv /venv && \
    /venv/bin/pip install --no-cache-dir --upgrade pip && \
    /venv/bin/pip install --no-cache-dir wheel && \
    /venv/bin/pip install --no-cache-dir -e /app/devito[extras,nvidia] && \
    # Need extra CFLAGS for mpi4py
    CFLAGS=-noswitcherror /venv/bin/pip install --no-cache-dir -r /app/devito/requirements-mpi.txt && \
    rm -rf ~/.cache/pip

RUN /venv/bin/jupyter serverextension enable dask_labextension 

## Environment Variables for OpenACC Builds
# Reference: https://github.com/devitocodes/devito/wiki/FAQ#can-i-manually-modify-the-c-code-generated-by-devito-and-test-these-modifications
# Set arch to PGI (pgcc)
ENV DEVITO_ARCH="nvc" 
ENV DEVITO_LANGUAGE="openacc"
ENV DEVITO_PLATFORM=nvidiaX
# Options: [unset, 1] For PGI openacc; Should only be set after a first execution of the benchmark
# ENV DEVITO_JIT_BACKDOOR=1 

# Enable logging, Options: [unset, PERF, DEBUG]
ENV DEVITO_LOGGING=DEBUG 
#ENV PGI_ACC_TIME=1 

# add remaining files to the container
ADD docker/run-jupyterlab.sh /jupyter
ADD docker/run-tests.sh /tests
ADD docker/run-print-defaults.sh /print-defaults
ADD docker/nvdashboard.json /app/nvdashboard.json
RUN chmod +x /print-defaults /jupyter /tests /docker-entrypoint.sh 

## Create App user 
# Set the home directory to our app user's home.
ENV HOME=/app
ENV APP_HOME=/app

# Create the home directory for the new app user.
# Create an app user so our program doesn't run as root.
# Chown all the files to the app user.
RUN mkdir -p /app && \
    groupadd -r app && \
    useradd -r -g app -d /app -s /sbin/nologin -c "Docker image user" app && \
    chown -R app:app $APP_HOME && \
    chown app:app /docker-entrypoint.sh && \
    chown app:app /print-defaults && \
    chown app:app /tests && \
    chown app:app /jupyter
    
# Change to the app user.
USER app
WORKDIR /app/devito

EXPOSE 8888
ENTRYPOINT ["/docker-entrypoint.sh"]
CMD ["/jupyter"]
