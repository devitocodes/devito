##############################################################
# This Dockerfile contains the NVidia HPC SDK (nvc, cuda, OpenMPI) for Devito
##############################################################
ARG base=ubuntu:22.04
ARG ver
ARG arch="nvc"

########################################################################
# Build base image with common env
########################################################################
FROM ${base} AS sdk-base

SHELL ["/bin/bash", "-c"]

ARG ver=nvhpc
ARG arch

COPY requirements-mpi.txt /tmp/requirements-mpi.txt

# Install Python and system dependencies
RUN if command -v apt-get >/dev/null; then \
        export DEBIAN_FRONTEND=noninteractive && \
        apt-get update && \
        apt-get install -y -q gpg apt-utils curl wget libnuma-dev cmake git \
            dh-autoreconf python3-venv python3-dev python3-pip \
            libncurses5-dev libncursesw5-dev libdrm-dev libsystemd-dev \
            python3-dbg gdb numactl tmux vim libgl1-mesa-glx libfftw3-dev; \
    else \
        dnf update -y && \
        dnf install -y epel-release && \
        dnf install -y python3 python3-devel python3-pip \
            git gpgme wget curl numactl-devel hwloc hwloc-devel cmake \
            gcc gcc-c++ gcc-gfortran make automake autoconf libtool which procps-ng \
            openssl-devel libusbx libusbx-devel libstdc++-static \
            ncurses-devel libdrm-devel systemd-devel gdb numactl tmux vim mesa-libGL fftw-devel && \
        (dnf install -y python3-debug || true) && \
        dnf clean all && rm -rf /var/cache/dnf; \
    fi

# Configure NVIDIA HPC SDK repositories and install toolchain
RUN if command -v apt-get >/dev/null; then \
        curl https://developer.download.nvidia.com/hpc-sdk/ubuntu/DEB-GPG-KEY-NVIDIA-HPC-SDK | gpg --yes --dearmor -o /usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg && \
        echo 'deb [trusted=yes, signed-by=/usr/share/keyrings/nvidia-hpcsdk-archive-keyring.gpg] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' | tee /etc/apt/sources.list.d/nvhpc.list && \
        apt-key update && apt-get update -y && \
        if [ "$ver" = "nvhpc" ]; then \
            apt-get install -y -q --allow-unauthenticated ${ver}; \
        else \
            year=$(echo $ver | cut -d "-" -f 2) && minor=$(echo $ver | cut -d "-" -f 3) && \
            wget https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64/nvhpc_${year}.${minor}_amd64.deb && \
            apt-get install --allow-unauthenticated -y -q ./nvhpc_${year}.${minor}_amd64.deb; \
        fi; \
    else \
        rpm --import https://developer.download.nvidia.com/hpc-sdk/RPM-GPG-KEY-NVIDIA-HPC-SDK && \
        curl -fsSL https://developer.download.nvidia.com/hpc-sdk/nvhpc.repo -o /etc/yum.repos.d/nvhpc.repo && \
        sed -i 's/$releasever/9/g' /etc/yum.repos.d/nvhpc.repo && \
        dnf update -y && \
        if [ -z "$ver" ] || [ "$ver" = "nvhpc" ]; then \
            dnf install -y nvhpc; \
        else \
            dnf install -y ${ver}; \
        fi && \
        dnf clean all && rm -rf /var/cache/dnf; \
    fi

# Install additional runtime dependencies and Node.js
RUN if command -v apt-get >/dev/null; then \
        curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --yes --dearmor -o /etc/apt/keyrings/nodesource.gpg && \
        echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_18.x nodistro main" | tee /etc/apt/sources.list.d/nodesource.list && \
        apt-get update && \
        apt-get install -y -q \
            liblapack-dev libblas-dev \
            libibverbs-dev libmlx4-1 libmlx5-1 ibutils \
            nodejs ffmpeg gcc-offload-nvptx \
            texlive-latex-extra texlive-fonts-recommended dvipng cm-super; \
    else \
        curl -fsSL https://rpm.nodesource.com/pub_18.x/nodesource-release-el9-1.noarch.rpm -o /tmp/nodesource.rpm && \
        rpm -i /tmp/nodesource.rpm && \
        dnf install -y nodejs ffmpeg texlive-scheme-basic rdma-core libibverbs && \
        rm -f /tmp/nodesource.rpm && \
        dnf clean all && rm -rf /var/cache/dnf; \
    fi

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility

# MPI ROOT USER DEFAULTS
ENV OMPI_ALLOW_RUN_AS_ROOT=1
ENV OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
ENV OMPI_MCA_rmaps_base_oversubscribe=1
ENV OMPI_MCA_btl_base_warn_component_unused=0
ENV OMPI_MCA_hwloc_base_binding_policy=""
ENV UCX_MEMTYPE_CACHE=no
ENV UCX_NET_DEVICES=all
ENV UCX_SHM_DEVICES=all
ENV UCX_ACC_DEVICES=all
ENV UCX_RNDV_THRESH=0
ENV UCX_RNDV_SCHEME=get_zcopy
ENV NCCL_UCX_RNDV_THRESH=0
ENV NCCL_UCX_RNDV_SCHEME=get_zcopy
ENV NCCL_PLUGIN_P2P=ucx
ENV MELLANOX_MOUNT_DRIVER=1

ENV UCX_TLS=cuda,cuda_copy,cuda_ipc,sm,shm,self
# For Baremetal, these flags are also available
#ENV UCX_TLS=cuda,cuda_copy,cuda_ipc,sm,shm,self,rc_x,gdr_copy

# Make simlink for path setup since ENV doesn't accept shell commands.
RUN export NVARCH=$(ls -1 /opt/nvidia/hpc_sdk/Linux_x86_64/ | grep '\.' | head -n 1) && \
    export CUDA_V=$(ls /opt/nvidia/hpc_sdk/Linux_x86_64/${NVARCH}/cuda/ | grep '\.') && \
    ln -sf /opt/nvidia/hpc_sdk/Linux_x86_64/${NVARCH} /opt/nvhpc && \
    ln -sf /opt/nvidia/hpc_sdk/Linux_x86_64/${NVARCH}/cuda/${CUDA_V}/extras/CUPTI /opt/CUPTI && \
    ln -sf /opt/nvidia/hpc_sdk/Linux_x86_64/comm_libs/${CUDA_V}/nvshmem /opt/nvhpc/comm_libs/nvshmem && \
    ln -sf /opt/nvidia/hpc_sdk/Linux_x86_64/comm_libs/${CUDA_V}/nccl /opt/nvhpc/comm_libs/nccl && \
    ln -sf /opt/nvidia/hpc_sdk/Linux_x86_64/${NVARCH}/cuda/${CUDA_V}/compute-sanitizer/compute-sanitizer /opt/nvhpc/compilers/bin/compute-sanitizer

# Starting nvhpc 23.5 and cuda 12.1, hpcx and openmpi are inside the cuda version folder, only the bin is in the comm_libs path
RUN export CUDA_V=$(/opt/nvhpc/cuda/bin/nvcc --version | sed -n 's/^.*release \([0-9]\+\.[0-9]\+\).*$/\1/p') && \
    ls /opt/nvhpc/comm_libs/${CUDA_V}/hpcx/ &&\
    if [ -d /opt/nvhpc/comm_libs/${CUDA_V}/hpcx ]; then \
        rm -rf /opt/nvhpc/comm_libs/hpcx && rm -rf /opt/nvhpc/comm_libs/openmpi4 && \
        ln -sf /opt/nvhpc/comm_libs/${CUDA_V}/hpcx /opt/nvhpc/comm_libs/hpcx && \
        ln -sf /opt/nvhpc/comm_libs/${CUDA_V}/openmpi4 /opt/nvhpc/comm_libs/openmpi4;\
    fi;
# Set base path based on version
ENV HPCSDK_HOME=/opt/nvhpc
ENV HPCSDK_CUPTI=/opt/CUPTI

# required for nvidia-docker v1
RUN echo "$HPCSDK_HOME/cuda/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/cuda/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/compilers/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/comm_libs/mpi/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_CUPTI/lib64" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "$HPCSDK_HOME/math_libs/lib64" >> /etc/ld.so.conf.d/nvidia.conf    
    
# Compiler, CUDA, and Library paths
# CUDA_HOME has been deprecated but keep for now because of other dependencies (@mloubout).
ENV CUDA_HOME $HPCSDK_HOME/cuda
ENV NVHPC_CUDA_HOME $HPCSDK_HOME/cuda
ENV CUDA_ROOT $HPCSDK_HOME/cuda/bin
ENV PATH $HPCSDK_HOME/compilers/bin:$HPCSDK_HOME/cuda/bin:$HPCSDK_HOME/comm_libs/mpi/bin:${PATH}
ENV LD_LIBRARY_PATH $HPCSDK_HOME/cuda/lib:$HPCSDK_HOME/cuda/lib64:$HPCSDK_HOME/compilers/lib:$HPCSDK_HOME/math_libs/lib64:$HPCSDK_HOME/comm_libs/mpi/lib:$HPCSDK_CUPTI/lib64:bitcomp_DIR:${LD_LIBRARY_PATH}
ENV CPATH $HPCSDK_HOME/comm_libs/mpi/include:$HPCSDK_HOME/comm_libs/nvshmem/include:$HPCSDK_HOME/comm_libs/nccl/include:$HPCSDK_HOME/math_libs/include:${CPATH}

# MPI
RUN rm -f  $HPCSDK_HOME/comm_libs/mpi && \
    ln -sf $HPCSDK_HOME/comm_libs/hpcx/latest/ompi $HPCSDK_HOME/comm_libs/mpi

# Install python dependencies inside virtual environment
RUN python3 -m venv /venv && \
    /venv/bin/pip install --no-cache-dir --upgrade pip && \
    /venv/bin/pip install --no-cache-dir -r https://raw.githubusercontent.com/devitocodes/devito/main/requirements-nvidia.txt && \
    source /opt/nvhpc/comm_libs/hpcx/latest/hpcx-init.sh && \
    hpcx_load && \
    MPICC=mpicc /venv/bin/pip install --no-cache-dir -r /tmp/requirements-mpi.txt && \
    /venv/bin/pip install --no-cache-dir jupyter && \
    /venv/bin/jupyter server extension enable dask_labextension && \
    rm -rf ~/.cache/pip

# Cleanup package caches
RUN if command -v apt-get >/dev/null; then \
        apt-get clean && apt-get autoclean && apt-get autoremove -y && rm -rf /var/lib/apt/lists/*; \
    else \
        dnf clean all && rm -rf /var/cache/dnf; \
    fi

EXPOSE 8888
CMD ["/bin/bash"]

########################################################################
# NVC for GPUs via OpenACC config
########################################################################
FROM sdk-base AS nvc

# Make devito env vars file and extras
ADD docker/nvdashboard.json /app/nvdashboard.json

ENV DEVITO_ARCH="nvc"
ENV DEVITO_PLATFORM="nvidiaX"
ENV DEVITO_LANGUAGE="openacc"

########################################################################
# NVC for GPUs via CUDA config
########################################################################
FROM nvc AS nvcc

ENV DEVITO_ARCH="cuda"
ENV DEVITO_PLATFORM="nvidiaX"
ENV DEVITO_LANGUAGE="cuda"

########################################################################
# NVC for CPUs config
########################################################################
FROM nvc AS nvc-host

ENV DEVITO_ARCH="nvc"
ENV DEVITO_PLATFORM="cpu64"
ENV DEVITO_LANGUAGE="openmp"
